import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical # to_categorical ì„í¬íŠ¸ ì¶”ê°€ (í•„ìˆ˜)
import os
import pickle # ğŸ‘ˆ í† í¬ë‚˜ì´ì € ì €ì¥ì„ ìœ„í•´ ì¶”ê°€

# ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# -------------------------------------------------------------------
# 1. ê²½ë¡œ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ (CNN ìŠ¤íƒ€ì¼ ìœ ì§€)
# -------------------------------------------------------------------

# (User Input, AI Response) ìŒ
CONVERSATIONS = [
    ("ì•ˆë…•?", "ì•ˆë…• ë°˜ê°€ì›Œìš”."),
    ("ë‚˜ ì˜¤ëŠ˜ ê¸°ë¶„ ì¢‹ì•„!", "ì™€ì•„, ì‹ ë‚˜ê² ë‹¤!"),
    ("í† ë¼ëŠ” ë­˜ ë¨¹ì„ê¹Œ?", "í† ë¼ëŠ” ë‹¹ê·¼ì„ ì¢‹ì•„í•´ìš”."),
    ("ë¡œë´‡ì´ ë­ì•¼?", "ì €ëŠ” ì—¬ëŸ¬ë¶„ì˜ ì¹œêµ¬, ë¡œë´‡ì´ì—ìš”."),
    ("ë‚˜ ì¡¸ë ¤", "ì ê¹ ëˆˆì„ ë¶™ì´ëŠ” ê²Œ ì¢‹ê² ì–´ìš”."),
    ("ê³ ë§ˆì›Œ!", "ë³„ë§ì”€ì„ìš”!")
] * 30 # í•™ìŠµ ì²´ê°ì„ ìœ„í•´ ë°ì´í„°ë¥¼ 30íšŒ ë°˜ë³µí•˜ì—¬ ëŠ˜ë¦¼

MAX_WORDS = 10000 
MAX_SEQUENCE_LENGTH = 15
EMBEDDING_DIM = 100 
LSTM_UNITS = 128 
EPOCHS = 30 # í•™ìŠµ ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ ì—í¬í¬ ì¡°ì •
BATCH_SIZE = 4 

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
save_model_dir = os.path.join(BASE_DIR, '..', 'save_models')
os.makedirs(save_model_dir, exist_ok=True) # save_models í´ë” ì—†ìœ¼ë©´ ìƒì„±

# -------------------------------------------------------------------
# 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° í† í°í™”
# -------------------------------------------------------------------

def prepare_data(conversations):
    """
    ëŒ€í™” ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ê³ , RNN í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    input_texts = [pair[0] for pair in conversations]
    target_texts = [pair[1] for pair in conversations]
    
    tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<unk>")
    tokenizer.fit_on_texts(input_texts + target_texts) 
    
    input_sequences = tokenizer.texts_to_sequences(input_texts)
    target_sequences = tokenizer.texts_to_sequences(target_texts)
    
    X = pad_sequences(input_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')
    Y_sequences = pad_sequences(target_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')

    # Yë¥¼ ì›-í•« ì¸ì½”ë”©í•˜ì—¬ RNNì˜ ì¶œë ¥ í˜•ì‹ì— ë§ì¶¤
    vocab_size = len(tokenizer.word_index) + 1
    Y = to_categorical(Y_sequences, num_classes=vocab_size)
                
    return X, Y, tokenizer, vocab_size

# -------------------------------------------------------------------
# 3. RNN ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
# -------------------------------------------------------------------

def build_rnn_model(vocab_size):
    """
    ê°€ì¥ ê¸°ë³¸ì ì¸ ì‹œí€€ìŠ¤ íˆ¬ ì‹œí€€ìŠ¤(Sequence-to-Sequence) êµ¬ì¡°ì˜ RNN ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
    """
    model = Sequential([
        Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),
        LSTM(LSTM_UNITS, return_sequences=True), 
        Dropout(0.2),
        Dense(vocab_size, activation='softmax')
    ])
    
    model.compile(optimizer='adam', 
                  loss='categorical_crossentropy', 
                  metrics=['accuracy'])

    # ğŸ‘ˆ ì´ ë¼ì¸ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì„ ìˆ˜ë™ìœ¼ë¡œ ë¹Œë“œí•©ë‹ˆë‹¤.
    # input_shapeëŠ” (ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” ì œì™¸í•˜ê³ ) (MAX_SEQUENCE_LENGTH) ì…ë‹ˆë‹¤.
    model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))
                  
    return model

# -------------------------------------------------------------------
# 4. í•™ìŠµ ì‹¤í–‰ ë° ì €ì¥ (ìˆœìˆ˜ í•™ìŠµ ë¡œì§)
# -------------------------------------------------------------------

def main_train_and_save():
    X, Y, tokenizer, vocab_size = prepare_data(CONVERSATIONS)
    model = build_rnn_model(vocab_size)
    
    # ëª¨ë¸ í•™ìŠµ (CNN ë•Œì™€ ë™ì¼í•œ model.fit() í•¨ìˆ˜ ì‚¬ìš©)
    print(f"[INFO] RNN ëª¨ë¸ í•™ìŠµ ì‹œì‘. íŒŒë¼ë¯¸í„° ìˆ˜: {model.count_params()}")
    model.fit(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)
    
    # ëª¨ë¸ ì €ì¥
    model_path = os.path.join(save_model_dir, 'rnn_conversation_model.h5')
    model.save(model_path)
    print(f"\n[INFO] ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
    
    # ğŸ‘ˆ ì¶”ë¡  ì‹œ í•„ìˆ˜! í† í¬ë‚˜ì´ì € ì €ì¥
    tokenizer_path = os.path.join(save_model_dir, 'rnn_tokenizer.pkl')
    with open(tokenizer_path, 'wb') as f:
        pickle.dump(tokenizer, f)
    print(f"[INFO] í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ: {tokenizer_path}")

if __name__ == "__main__":
    main_train_and_save()